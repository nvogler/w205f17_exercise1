wget https://github.com/nvogler/w205f17_exercise1/blob/master/effective_care.csv
wget https://github.com/nvogler/w205f17_exercise1/blob/master/hospitals.csv
wget https://github.com/nvogler/w205f17_exercise1/blob/master/measures.csv
wget https://github.com/nvogler/w205f17_exercise1/blob/master/readmissions.csv
wget https://github.com/nvogler/w205f17_exercise1/blob/master/survey_responses.csv

/root/start-hadoop.sh
/data/start_postgres.sh

/root/stop-hadoop.sh
/data/stop_postgres.sh

setx SPARK_HOME C:\spark-2.1.0-bin-hadoop2.7
setx HADOOP_HOME C:\spark-2.1.0-bin-hadoop2.7
setx PYSPARK_DRIVER_PYTHON ipython
setx PYSPARK_DRIVER_PYTHON_OPTS notebook

;C:\spark-2.1.0-bin-hadoop2.7\bin

git config --global user.name "Nic"
git config --global user.email nvogler@umich.edu

git clone https://github.com/UC-Berkeley-I-School/w205-labs-exercises.git

mount -t ext4 /dev/xvdc /data
/root/start-hadoop.sh
/data/start_postgres.sh

export SPARK=/home/w205/spark15
export SPARK_HOME=$SPARK
export PATH=$SPARK/bin:$PATH

x = [1,2,3,4,5,6,7,8,9];
distData = sc.parallelize(x);
print distData;

crimedata=sc.textFile("file:///home/w205/w205-fall-17-labs-exercises/data/Crimes_-_2001_to_present_data/Crimes.csv")

from pyspark.sql import SQLContext
from pyspark.sql.types import *
sqlContext = SQLContext(sc)
lines = sc.textFile('file:///home/w205/w205-fall-17-labs-exercises/data/weblog_lab.csv')
parts = lines.map(lambda l: l.split('\t'))
Web_Session_Log = parts.map(lambda p: (p[0],p[1],p[2], p[3], p[4]))
schemaString = 'DATETIME USERID SESSIONID PRODUCTID REFERERURL'
fields = [StructField(field_name, StringType(), True) for field_name in schemaString.split()]
schema = StructType(fields)
schemaWebData = sqlContext.createDataFrame(Web_Session_Log, schema)
schemaWebData.registerTempTable('Web_Session_Log')
results = sqlContext.sql("""SELECT count(*) FROM Web_Session_Log WHERE 'refereurl'= 'http://www.ebay.com'""")

select count(*) from web_session_log where 'refereurl'= "http://www.ebay.com" ;

